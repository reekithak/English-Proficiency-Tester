{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DOC-SIMILARITY MODULE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:36:23.249462Z",
     "start_time": "2021-04-22T20:36:23.230460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "file_docs = []\n",
    "\n",
    "#Tokenize\n",
    "with open ('demofile.txt',encoding='utf-8') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file_docs.append(line)\n",
    "\n",
    "print(\"Number of documents:\",len(file_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:36:23.841286Z",
     "start_time": "2021-04-22T20:36:23.832293Z"
    }
   },
   "outputs": [],
   "source": [
    "#tokenize and create dict\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in file_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:36:24.467098Z",
     "start_time": "2021-04-22T20:36:24.448139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'by': 1, 'calculated': 2, 'calculating': 3, 'distance': 4, 'document': 5, 'is': 6, 'similarity': 7, '(': 8, ')': 9, 'a': 10, 'and': 11, 'angle': 12, 'are': 13, 'as': 14, 'between': 15, 'concept': 16, 'documents': 17, 'given': 18, 'the': 19, 'treated': 20, 'two': 21, 'vectors': 22, 'where': 23, 'words': 24, 'frequency': 25, 'in': 26, 'occurrences': 27, 'of': 28, ':': 29, 'an': 30, 'example': 31, 'let': 32, 's': 33, 'see': 34, '’': 35}\n"
     ]
    }
   ],
   "source": [
    "#tokenize and create dict\n",
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary.token2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:36:25.303783Z",
     "start_time": "2021-04-22T20:36:25.286784Z"
    }
   },
   "outputs": [],
   "source": [
    "#BOW\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:36:25.755518Z",
     "start_time": "2021-04-22T20:36:25.742482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.', 0.1], ['by', 0.5], ['calculated', 0.25], ['calculating', 0.5], ['distance', 0.25], ['document', 0.21], ['is', 0.25], ['similarity', 0.5]]\n",
      "[['.', 0.05], ['calculated', 0.12], ['distance', 0.12], ['document', 0.1], ['is', 0.24], ['(', 0.24], [')', 0.24], ['a', 0.12], ['and', 0.24], ['angle', 0.24], ['are', 0.12], ['as', 0.47], ['between', 0.24], ['concept', 0.24], ['documents', 0.24], ['given', 0.12], ['the', 0.12], ['treated', 0.24], ['two', 0.24], ['vectors', 0.24], ['where', 0.24], ['words', 0.12]]\n",
      "[['.', 0.07], ['document', 0.14], ['a', 0.17], ['are', 0.17], ['given', 0.17], ['the', 0.17], ['vectors', 0.17], ['words', 0.17], ['frequency', 0.34], ['in', 0.34], ['occurrences', 0.34], ['of', 0.68]]\n",
      "[[':', 0.38], ['an', 0.38], ['example', 0.38], ['let', 0.38], ['s', 0.38], ['see', 0.38], ['’', 0.38]]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "import numpy as np\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:36:26.283281Z",
     "start_time": "2021-04-22T20:36:26.278281Z"
    }
   },
   "outputs": [],
   "source": [
    "#Similarity Index Measure\n",
    "# building the index\n",
    "sims = gensim.similarities.Similarity('./',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:36:26.920205Z",
     "start_time": "2021-04-22T20:36:26.911174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4\n"
     ]
    }
   ],
   "source": [
    "file2_docs = []\n",
    "\n",
    "with open ('demofile2.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file2_docs.append(line)\n",
    "\n",
    "print(\"Number of documents:\",len(file2_docs))  \n",
    "for line in file2_docs:\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc) \n",
    "    #update an existing dictionary and create bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:36:27.749728Z",
     "start_time": "2021-04-22T20:36:27.727724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Result: [0.         0.         0.         0.75592893]\n"
     ]
    }
   ],
   "source": [
    "# perform a similarity query against the corpus\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "# print(document_number, document_similarity)\n",
    "print('Comparing Result:', sims[query_doc_tf_idf]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:36:28.695030Z",
     "start_time": "2021-04-22T20:36:28.679057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75592893\n"
     ]
    }
   ],
   "source": [
    "#Average Similarity\n",
    "import numpy as np\n",
    "\n",
    "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "print(sum_of_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:36:30.108738Z",
     "start_time": "2021-04-22T20:36:30.095711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity float: 0.18898223340511322\n",
      "Average similarity percentage: 18.898223340511322\n",
      "Average similarity rounded percentage: 19\n"
     ]
    }
   ],
   "source": [
    "percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
    "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
    "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
    "print(f'Average similarity rounded percentage: {percentage_of_similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:36:33.412289Z",
     "start_time": "2021-04-22T20:36:33.400254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Document similarity is calculated by calculating document distance.',\n",
       " 'Document distance is a concept where words(documents) are treated as vectors and is calculated as the angle between two given document vectors.',\n",
       " 'Document vectors are the frequency of occurrences of words in a given document.',\n",
       " 'Letâ€™s see an example:']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file2_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DIRECT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:36:34.517947Z",
     "start_time": "2021-04-22T20:36:34.494947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Result: [1.0000001  0.14399059 0.03662721 0.        ]\n",
      "avg: 0.29515448212623596\n",
      "Comparing Result: [0.14399059 1.         0.1570631  0.        ]\n",
      "avg: 0.32526344060897827\n",
      "Comparing Result: [0.03662721 0.1570631  1.         0.        ]\n",
      "avg: 0.29842257499694824\n",
      "Comparing Result: [0.         0.         0.         0.75592893]\n",
      "avg: 0.18898223340511322\n"
     ]
    }
   ],
   "source": [
    "avg_sims = [] # array of averages\n",
    "\n",
    "# for line in query documents\n",
    "for line in file2_docs:\n",
    "    \n",
    "    # tokenize words\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    # create bag of words\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "    # find similarity for each document\n",
    "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "    # print (document_number, document_similarity)\n",
    "    print('Comparing Result:', sims[query_doc_tf_idf]) \n",
    "    # calculate sum of similarities for each query doc\n",
    "    sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "    # calculate average of similarity for each query doc\n",
    "    avg = sum_of_sims / len(file_docs)\n",
    "    # print average of similarity for each query doc\n",
    "    print(f'avg: {sum_of_sims / len(file_docs)}')\n",
    "    # add average values into array\n",
    "    avg_sims.append(avg)  \n",
    "# calculate total average\n",
    "total_avg = np.sum(avg_sims, dtype=np.float)\n",
    "# round the value and multiply by 100 to format it as percentage\n",
    "percentage_of_similarity = round(float(total_avg) * 100)\n",
    "# if percentage is greater than 100\n",
    "# that means documents are almost same\n",
    "if percentage_of_similarity >= 100:\n",
    "    percentage_of_similarity = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:36:35.246936Z",
     "start_time": "2021-04-22T20:36:35.231931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75592893\n"
     ]
    }
   ],
   "source": [
    "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "print(sum_of_sims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
