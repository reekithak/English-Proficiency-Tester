{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T18:43:45.458198Z",
     "start_time": "2021-05-04T18:43:44.270348Z"
    }
   },
   "source": [
    "**Google Default Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T04:59:17.750972Z",
     "start_time": "2021-05-05T04:59:14.437882Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T04:59:17.765951Z",
     "start_time": "2021-05-05T04:59:17.750972Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = r'D:\\working repos\\English-Proficiency-Tester\\Modules\\AI-Modules\\1.Doc-Text-Sim\\5-4-21\\data\\GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:00:18.783066Z",
     "start_time": "2021-05-05T04:59:17.770962Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:38:02.464078Z",
     "start_time": "2021-05-05T05:38:02.443071Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class DocSim:\n",
    "    def __init__(self, w2v_model, stopwords=None):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.stopwords = stopwords if stopwords is not None else []\n",
    "\n",
    "    def vectorize(self, doc: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Identify the vector values for each word in the given document\n",
    "        :param doc:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        doc = doc.lower()\n",
    "        words = [w for w in doc.split(\" \") if w not in self.stopwords]\n",
    "        word_vecs = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                vec = self.w2v_model[word]\n",
    "                word_vecs.append(vec)\n",
    "            except KeyError:\n",
    "                # Ignore, if the word doesn't exist in the vocabulary\n",
    "                pass\n",
    "\n",
    "        # Assuming that document vector is the mean of all the word vectors\n",
    "        # PS: There are other & better ways to do it.\n",
    "        vector = np.mean(word_vecs, axis=0)\n",
    "        return vector\n",
    "\n",
    "    def _cosine_sim(self, vecA, vecB):\n",
    "        \"\"\"Find the cosine similarity distance between two vectors.\"\"\"\n",
    "        csim = np.dot(vecA, vecB) / (np.linalg.norm(vecA) * np.linalg.norm(vecB))\n",
    "        if np.isnan(np.sum(csim)):\n",
    "            return 0\n",
    "        return csim\n",
    "\n",
    "    def calculate_similarity(self, source_doc, target_docs=None, threshold=0):\n",
    "        \"\"\"Calculates & returns similarity scores between given source document & all\n",
    "        the target documents.\"\"\"\n",
    "        if not target_docs:\n",
    "            return []\n",
    "\n",
    "        if isinstance(target_docs, str):\n",
    "            target_docs = [target_docs]\n",
    "\n",
    "        source_vec = self.vectorize(source_doc)\n",
    "        results = []\n",
    "        for doc in target_docs:\n",
    "            target_vec = self.vectorize(doc)\n",
    "            sim_score = self._cosine_sim(source_vec, target_vec)\n",
    "            if sim_score > threshold:\n",
    "                results.append({\"score\": sim_score, \"doc\": doc})\n",
    "            # Sort results by score in desc order\n",
    "            results.sort(key=lambda k: k[\"score\"], reverse=True)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:38:03.029661Z",
     "start_time": "2021-05-05T05:38:03.012649Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = DocSim(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:38:04.159517Z",
     "start_time": "2021-05-05T05:38:04.148517Z"
    }
   },
   "outputs": [],
   "source": [
    "source_doc = \"Nature has furnished us with numerous significant assets like water, air, food, therapeutic plants, fire, fuels, woods, and trees, and so forth. Every one of these assets helps us for different purposes. As a whole, we realize that our endurance will turn out to be difficult or end without them. As they serve us, they likewise need our adoration and service.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:38:04.472568Z",
     "start_time": "2021-05-05T05:38:04.460571Z"
    }
   },
   "outputs": [],
   "source": [
    "target_docs = [\"Nature has furnished us with numerous significant assets like water, air, food, therapeutic plants, fire, fuels, woods, and trees, and so forth. Every one of these assets helps us for different purposes. As a whole, we realize that our endurance will turn out to be difficult or end without them. As they serve us, they likewise need our adoration and service.\"]\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:38:05.407518Z",
     "start_time": "2021-05-05T05:38:05.397522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 1.0, 'doc': 'Nature has furnished us with numerous significant assets like water, air, food, therapeutic plants, fire, fuels, woods, and trees, and so forth. Every one of these assets helps us for different purposes. As a whole, we realize that our endurance will turn out to be difficult or end without them. As they serve us, they likewise need our adoration and service.'}]\n"
     ]
    }
   ],
   "source": [
    "sim_scores = ds.calculate_similarity(source_doc, target_docs)\n",
    "\n",
    "print(sim_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:38:29.573608Z",
     "start_time": "2021-05-05T05:38:29.556610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_scores[0]['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternate Method - TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:38:52.450883Z",
     "start_time": "2021-05-05T05:38:52.436887Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:38:53.180905Z",
     "start_time": "2021-05-05T05:38:53.162772Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_tfidf_similarity(source_doc,target_docs):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # To make uniformed vectors, both documents need to be combined first.\n",
    "    target_docs.insert(0, source_doc)\n",
    "    embeddings = vectorizer.fit_transform(target_docs)\n",
    "\n",
    "    cosine_similarities = cosine_similarity(embeddings[0:1], embeddings[1:]).flatten()\n",
    "\n",
    "    highest_score = 0\n",
    "    highest_score_index = 0\n",
    "    for i, score in enumerate(cosine_similarities):\n",
    "        if highest_score < score:\n",
    "            highest_score = score\n",
    "            highest_score_index = i\n",
    "\n",
    "    return highest_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:14:56.040448Z",
     "start_time": "2021-05-05T05:14:55.561679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999997"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_tfidf_similarity(source_doc,target_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
